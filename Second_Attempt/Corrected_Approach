# Step-by-Step Implementation: Kullmann ML Approach in R
# Samira Salimiyan -- CBC class Fall 2025 Assignment 5

##### Setup
install.packages(c("glmnet", "randomForest", "pROC"))
library(glmnet)
library(randomForest)
library(pROC)

setwd("C:/Users/szs0394/Downloads/05")

##### Step 1: Create Simulated Data
set.seed(123)

n_samples <- 200
n_features <- 5000
n_signal <- 20   # fewer true signals for realism

# Outcome labels
y <- factor(rep(c("Control", "Case"), each = n_samples/2))

# Generate background noise features ~ normal(0.5, 0.1)
X <- matrix(rnorm(n_samples * n_features, mean = 0.5, sd = 0.1), 
            nrow = n_samples, ncol = n_features)

# Add weaker signal to first 20 features
for(i in 1:n_signal) {
  effect <- runif(1, 0.02, 0.05)  # smaller signal shift
  X[y == "Case", i] <- X[y == "Case", i] + effect
}

# Keep values bounded in [0,1]
X[X > 1] <- 1
X[X < 0] <- 0
colnames(X) <- paste0("Feature_", 1:n_features)

##### Step 2: Elastic Net Feature Selection
set.seed(42)
train_idx <- sample(1:nrow(X), size = 0.7 * nrow(X))
X_train <- X[train_idx, ]
y_train <- y[train_idx]
X_test <- X[-train_idx, ]
y_test <- y[-train_idx]

cv_fit <- cv.glmnet(X_train, y_train, family = "binomial", alpha = 0.5, nfolds = 10)
coefs <- coef(cv_fit, s = "lambda.min")
selected_features <- which(coefs[-1] != 0)

X_train_selected <- X_train[, selected_features, drop = FALSE]
X_test_selected <- X_test[, selected_features, drop = FALSE]

##### Step 3: Random Forest
rf_model <- randomForest(X_train_selected, y_train, ntree = 500, importance = TRUE)

##### Step 4: Performance Evaluation
create_confusion_matrix <- function(predicted, actual) {
  predicted <- factor(predicted, levels = c("Control", "Case"))
  actual <- factor(actual, levels = c("Control", "Case"))
  cm <- table(Predicted = predicted, Actual = actual)
  TN <- cm[1,1]; FP <- cm[1,2]; FN <- cm[2,1]; TP <- cm[2,2]
  accuracy <- (TP + TN) / sum(cm)
  sensitivity <- TP / (TP + FN)
  specificity <- TN / (TN + FP)
  precision <- TP / (TP + FP)
  return(list(matrix = cm, accuracy = accuracy,
              sensitivity = sensitivity,
              specificity = specificity,
              precision = precision))
}

rf_pred <- predict(rf_model, X_test_selected)
cm_results <- create_confusion_matrix(rf_pred, y_test)
accuracy <- cm_results$accuracy

pred_prob <- predict(rf_model, X_test_selected, type = "prob")[,2]
roc_obj <- roc(y_test, pred_prob, quiet = TRUE)
auc_score <- auc(roc_obj)

##### Step 5: Save Outputs
sink("results_summary.txt")
cat("=== RESULTS ===\n")
cat("Accuracy:", round(accuracy, 3), "\n")
cat("AUC:", round(auc_score, 3), "\n")
cat("Sensitivity:", round(cm_results$sensitivity, 3), "\n")
cat("Specificity:", round(cm_results$specificity, 3), "\n")
cat("Precision:", round(cm_results$precision, 3), "\n")
cat("Features selected:", length(selected_features), "\n\n")
cat("Confusion Matrix:\n")
print(cm_results$matrix)
sink()

write.csv(as.data.frame(cm_results$matrix), "confusion_matrix.csv")

png("ROC_curve.png", width = 800, height = 600)
plot(roc_obj, main = "ROC Curve")
text(0.4, 0.2, paste("AUC =", round(auc_score, 3)))
dev.off()

importance_scores <- importance(rf_model)[,1]
top_features <- head(order(importance_scores, decreasing = TRUE), 10)

png("feature_importance.png", width = 800, height = 600)
barplot(importance_scores[top_features], 
        names.arg = paste0("F", selected_features[top_features]),
        main = "Top 10 Important Features",
        las = 2)
dev.off()

write.csv(importance_scores, "feature_importance_scores.csv")
